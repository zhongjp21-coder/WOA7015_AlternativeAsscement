# 中文文档
参考spectrum模板
## ABSTRACT
（includeing）
- Introduction
- Method
- Results
- Conclusion
- Word Count - Max300 words
- Keywords 5-6 keywords

## INTRODUCTION
From preliminary report,oOur research problem is how to quantitatively evaluate the performance of closed-ended and open-ended questions, thereby responding to patients' fundamental demands for "understanding medical images", while ensuring clinical availability and safety boundaries.
Based on the research problem, we have proposed research questions and objectives in two aspects: closed-end performance comparison, open-end expression and safety.

⚫ RQ1: Which model, baseline or VLM, performs better in answering closed-ended questions about radiology images? How big is the gap between the two models?

➢ RO1: Construct a unified evaluation framework, calculate closed classification indicators (Overall Acc, Macro-F1), calculate the gap, and evaluate and output the classification results under the two routes respectively.

⚫ RQ2: How is the expression quality of large visual language models(VLM) in medical open-ended question answering? What is the risk that the content it generates may cause hallucinations?

➢ RO2: Use language similarity metrics (BLEU, ROUGE-L, METEOR, etc.) and human assistance to evaluate the expression quality of open-ended questions. Conduct sensitivity analysis on repetitive questions, systematically quantify the expression quality and illusion risk of answers generated by vlm, and formulate specific operation guidelines to reduce risks.
【说明我们本次实验使用的数据集、baseline方法、和Van Sonsbeek方法】
## METHODS

### 2.1 Dataset
本研究采用 VQA-RAD 数据集开展医学视觉问答任务。VQA-RAD 包含 315 张放射医学图像（包括 X 光和 CT），以及 2,247 对经临床医生验证的问题–答案（QA）样本。与依赖外部医学知识库的数据集（如 SLAKE）不同，VQA-RAD 主要关注“纯视觉”问题，即答案可直接从图像内容中推断。这一特性使其适合用于评估模型的视觉表征学习能力及视觉–语言对齐能力。

数据集中问题类型包括封闭式问题和开放式问题，其中封闭式问题占 57.7%，开放式问题占 42.3%。问题内容涵盖异常存在、解剖位置以及成像模态等多个医学方面。需要注意的是，约 19.6% 的问题存在重复，这对模型评估提出了更高要求，以避免模型过度依赖文本模式而非视觉信息。

在实验设置中，数据集按照 80:20 的比例随机划分为训练集和测试集。所有图像均被调整为 
224
×
224
224×224 像素，并进行归一化处理，以满足预训练 CLIP 图像编码器的输入要求。文本数据经过分词处理，并被组织为因果语言建模格式（如“Question: [Q] Answer: [A]”），以支持生成式学习过程。
配图1：关于VQA-RAD 数据集统计信息
配图2: 数据处理流程示意图
=======
#### **2.2 baseline**
本模块将baseline的系统的架构示意图，分为Data Preprocessing、Data Transformation and Loading、Model Building、Training Pipeline四个模块，具体功能参考示意图。以下各小节对每一个单独模块和最终的训练设备做出说明。


2.1.1 Data Preprocessing
数据集预处理分为4个步骤，数据集加载与预处理、词汇表构建、答案映射表构建、数据集对象，参考下图。
数据集加载与预处理，首选读取原始文件VQA_RAD Dataset Public.json，原始文件包含JSON 结构，每个样本包含：image_name：图片文件名、question：医学影像相关的问题、answer：对应答案（文本形式）、answer_type：问题类型（OPEN/CLOSED）等信息。
检查文件，移除缺失关键字段（image_id, question, answer）的样本，经检查，当前数据集无缺失。将数据集按70%，15%，15%分割为训练，验证，测试集，随机种子设置为42。经过预处理，数据集问题类型分布参考下表。
词汇表构建的核心目的是将文本问题转换为机器可处理的数值形式，为模型提供固定维度的标准化输入。我们仅使用训练集中的问题文本构建词汇表，构建前先将问题转换为小写，按空格分词，移除标点符号，设置最小词频阈值：min_freq=2。将对应的词做特殊标记：<PAD>：填充符（ID: 0），<UNK>：未知词为低频词（出现1次）（ID: 1），<SOS>：句首标记（ID: 2），<EOS>：句尾标记（ID: 3）。
经过处理统计得到去重后训练集问题总词数2,158 个，包含特殊标记的词汇表大小1,284 个。下表为部分高频词汇统计结果。
答案映射表构建的核心目的是将文本答案转换为分类任务的类别标签，实现开放域答案到固定类别空间的标准化映射。为避免数据泄露，仅使用训练集中的答案构建映射表，将答案视为分类标签，多类别分类任务。由于映射表仅基于训练集答案构建，因此测试集中可能出现的训练集未见答案无法映射。 为此专门添加一个 <UNK>（未知）类别，当验证集或测试集中的答案不在训练集答案集合中时，统一映射到此类别，确保所有样本都有有效标签，同时真实反映模型处理新答案的能力。
经统计，训练集唯一答案数为1,412 个，高频答案参考下表，表中显示答案呈现长尾分布，类别高度不均衡。
数据集对象的目的是封装数据加载与预处理逻辑，提供标准化的样本访问接口。 它统一处理图像变换、文本序列化和标签编码，确保数据格式符合模型输入要求。同时支持PyTorch DataLoader的批量加载、多进程处理和数据打乱，实现高效的数据流水线。

经过数据预处理后我们获得以下文件清单：
├── train_split.csv           # 训练集（2,671条）
├── val_split.csv             # 验证集（572条）
├── test_split.csv            # 测试集（572条）
├── vocab.json                # 词汇表映射（1,284个词）
└── answer_mapping.json       # 答案映射表（1,413个类别）

2.1.2 Data Transformation and Loading
Data Transformation 分为Image transformations和文本序列化。图像预处理流程首先使用Resize(224, 224)统一尺寸，之后使用参数均值[0.485, 0.456, 0.406] 和 std[0.229, 0.224, 0.225]将图像像素值归一化到接近正态分布。最终得到 结构为(3*224*224)的归一化图像张量。
文本序列化仅应用于问题文本，按照最大序列长度max_seq_len=25统一所有问题长度， 短于25词的用<PAD>填充，长于25词的截断前25词，根据词汇表将每个词映射为对应ID，不在词汇表的词映射为<UNK>（ID:1）。截断时保留前25词是基于医学问题通常关键信息在前部的假设。
答案在数据集中已通过答案映射表转换为单个类别ID，作为分类标签直接使用，无需序列化处理。
数据转化完成后，对图像存在性进行检查，如有缺失图像，用零张量替代并记录，同时确保answer_type规范化为CLOSED/OPEN。
DataLoader将原始数据集转换为可批量迭代的数据流，为模型训练提供标准化的、高效的数据供给。baseline模型以每批样本数32，加载进程数为2的配置（使用GPU情况下）进行加载。其中shuffle参数训练集设置为True，每个epoch随机打乱，防止过拟合，验证/测试设置为False，便于跟踪性能和确保可复现性。

2.1.3 Model Building
MedVQA_ResNet_LSTM 整体架构是一个双流融合网络，它使用ResNet50作为图像编码器来完成视觉特征提取，使用LSTM + Embedding作为文本编码器来实现语义特征提取，最终使用MLP作为融合分类器执行多模态决策。
ResNet是一个深度卷积神经网络，用于处理图像数据。其主要任务是从图像中提取特征。在对VQA-RAD的输入图像进行预处理后，通过ResNet提取高层次图像特征，输出的高维特征将被传递到融合模块。
LSTM是一种循环神经网络（RNN），用于处理序列数据，比如文本，它能够处理输入的自然语言问题（如“这是什么疾病？”）并提取语义特征。输入到LSTM的文本（问题）首先会通过嵌入层（embedding layer）将单词转化为向量表示，并处理成定长的文本序列。
图像和文本特征被分别提取后，会通过特征融合模块合并。在本架构中，将这个过程分为2步，第一步，拼接：将图像特征和文本特征直接拼接在一起生成一个联合特征，联结后的特征维度会增加。第二步，使用MLP（多层感知机）实现非线性变换，通过若干个全连接层（Linear Layer），对拼接后的联合特征进行更高阶的特征提取，最后输出分类结果。
以上为baseline模型的架构，其中用到的超参信息下表。


2.1.4 Training Pipeline
模型的训练与验证基于PyTorch框架实现，训练采用经典的训练-验证迭代策略。初始化后，在每一个训练周期（epoch）中，首先模型接收训练数据，并使用前向传播计算预测结果，之后使用交叉熵损失函数 (Cross Entropy Loss) 计算模型的预测误差，接着基于误差反向传播更新模型参数，最后在验证集上评估模型，记录损失和其他性能指标。
其中为了防止模型在训练过程中发生过拟合，我们引入了早停机制（Early Stopping）。在验证阶段，我们将通过监控验证集损失（Validation Loss）判断模型性能。如果验证损失连续 5个周期（5为设定的耐心值）未能改善，训练将自动停止，从而减少训练时间并提高泛化能力。
训练的目标是使模型在降低验证集损失的同时提升验证集的性能。每轮训练结束后，我们会保存当前模型性能最优的权重，确保最终能够选取效果最好的模型。
以下是训练过程中使用的关键超参数：
- **批量大小（Batch Size）**: 设置为16，保证训练效率与GPU显存占用之间取得平衡。
- **学习率（Learning Rate）**: 初始学习率设置为0.0001，并在训练过程中使用学习率衰减策略（ReduceLROnPlateau），根据验证集性能动态调整学习率。
- **训练周期数（Epochs）**: 训练30个周期，结果表明，模型在15-25个周期通常会获得收敛效果。
为了全面评估模型性能，我们在训练和验证阶段使用以下指标：
- **交叉熵损失**：作为主要优化目标，用于衡量模型预测与真实标签之间的差异。
- **准确率（Accuracy）**：用于评估模型在验证集上的分类性能。
- **F1-Score** 和 **Precision**：用于更全面地衡量模型在不平衡数据集上的分类性能。



   
        
#### **2.3 Van Sonsbeek模型**（CLIP + GPT-2 + Mapping Network）

## RESULTS

### generative model
首先是训sonsbeek的训练过程的results，可以看到早期的loss曲线快速下降,最显著的特征是epoch0到epoch1的垂直下降，这完全符合sonsbeek等人架构的预期，由于visionEncoder(clip)和languistic model（gpt-2）都是预训练好并且被冻结的，他们已经具备了非常强的特征提取和语言生成能力，模型不需要从0开始旭熙，mapping network只需要学习一个简单的线性变化来对齐视觉和语言，因此模型在初期训练能迅速找到最优解的大致方向。
从ecpoch5开始，曲线进入plateau，loss稳定在0.3左右，并且线条十分平滑，没有剧烈震荡，这个平滑的曲线恰好说明了lr=1e-4和优化器adamW设置得比较合适，没有出现梯度爆炸和震荡。这种loss曲线过早平滑也暗示了vqa-rad数据集比较小，模型能很快记住训练集中的所有简单映射规则，并且，mapping network是一个简单mlp，容量有限，并且主干被冻结了，很难在这种不解冻主干的情况下挖掘出更深层的语义规律。
总的来说，这张图说明了我们的代码是可行的，并且
![final_training_loss.png](../llm_results/final_training_loss.png)


3.1.1 baseline 结果
baseline训练集: 1,574个样本 (70%)验证集: 337个样本 (15%)测试集: 337个样本 (15%)。
硬件信息为colab T4 GPU  系统 RAM 2.4 / 12.7 GB  GPU RAM 0.3 / 15.0 GB 磁盘 38.8 / 112.6 GB
下表为其他关键超参数。
3.1.2 

## DISCUSSION
## CONCLUSION
## ACKNOWLEDGEMENT
## AUTHORS CONTRIBUTION
## REFERENCES (统一插入即可,先不写，zotero有统一插入功能)
## APENDIX