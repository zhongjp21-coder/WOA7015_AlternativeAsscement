# Slide scripts for presentation
[15s]hi, everyone. Today our presentation topic is A Comparative Study of CNN Baseline Classifier and LLM-Based Model for Medical Visual Question Answering.
## introduction: problems
[32s]It is  easy for patients and clinicians to access medical images now. However, how to effectively interpret these image information to support diagnosis and decision-making has become a key demand.
Visual question answering (VQA) has demonstrated potential in general fields, but when applied to the field of radiology, it faces unique challenges such as clinical reliability and safety.
## ro&&rq
[50s ]Against this backdrop, we have raised two core research questions: First, Which model performs better in the baseline model and the VLM model?
What is the expression quality  and the risk of hallucinationis  of the VLM model in open-ended questions.
To address these questions, we have set clear research goals: to evaluate the results of closed-end questions through overall accuracy, macro F1;
Meanwhile,  to evaluate the expression quality of open-ended questions by language similarity indicators such as BLEU and ROUGE-L.
## methods: baseline model
[45s]Next, let's introduce the research methods in detail.
First  is the construction of the baseline model, It's divided into four core modules.
In the data preprocessing stage, After cleaning data, We divided the data  into the training set, validation set and test set at a percentage of 70%, 15% and 15% respectively.
Afterwards, we constructed a vocabulary list based on the question texts in the training set. Meanwhile, we built an answer mapping table based on the answers in the training set, converting the text answers into classification labels.

[43s]During the data Transformation  and loading process, images are resized and standardized, while text is processed with a maximum sequence length of 25 as the standard, and then mapped to the corresponding ID. Eventually, all are converted into  tensors.
Subsequently, a batch iterative data stream is constructed through DataLoader. Among them, the training set is shuffled  in order to prevent overfitting, while the validation set and test set remain in order for easy result tracking.

[37s] The baseline model adopts the MedVQA_ResNet_LSTM dual-stream fusion architecture. ResNet50 is used to extract the advanced visual features of the image, and Embedding+LSTM is used to extract the semantic features of the text. Then, after concatenating the two types of features, MLP is used for nonlinear transformation, and finally we have the classification result.

[45s]The training process is based on the PyTorch framework, adopts a training-validation iterative strategy, and also introduces an early stop mechanism.
stage1 uses the training set to train the model, while stage2 uses the validation set to verify the model's accuracy and F1. At the same time, if there is no improvement in the validation loss for N consecutive cycles, the training is stopped to avoid overfitting.
The batch size of the model is 16, and LR is dynamically adjusted using ReduceLROnPlateau.

## methods: generative model
## 
## results: baseline model
[90s]The following are the core results of this research.
First, let's look at the baseline model. Its training accuracy rate reaches 78%, and the training loss is as low as 0.66, indicating that the model has basic fitting ability on the training data and the configuration is reasonable.
However, the accuracy rate of the validation set is only 44%, and test set is as low as 33%. The gap between the training and test accuracy rates is as high as 44%, indicating a serious overfitting phenomenon.
The F1 score, precision rate, recall rate are also generally low, at 13.64%, 14.54% and 13.27% respectively, reflecting the significant impact of category imbalance on prediction quality.
From the perspective of problem types, the model has the highest accuracy rate on closed-end problems, reaching 48%, but the comprehensive evaluation index is still at a low level, indicating that it only performs slightly better on simple judgment problems, and its overall generalization ability is insufficient.

## results: generative model

## comparison


## conclusion









âš¡ Training Characteristics
Parameter Efficiency: Only the mapping network is trained (~1% of parameters)
Loss Calculation: Only text tokens contribute to loss; visual prefixes are masked
Objective: Causal language modeling, learning cross-modal alignment
Advantages: Reduces computational complexity, prevents overfitting on small datasets


ğŸ’¡ Summary of Evaluation Principles
Task Adaptability: Different evaluation metrics for different question types

Medical Specificity: Considers clinical validity and semantic accuracy

Benchmark Consistency: Follows Med-VQA domain standard practices

Comprehensiveness: Multi-dimensional assessment of model capabilities

# Generative Model Experimental Results Analysis
Page 1: Training Performance and Overall Results
ğŸ“‰ Training Loss Curve Characteristics
Rapid Initial Decline: Sharp drop from Epoch 0 to Epoch 1

Cause Analysis:

Both CLIP encoder and GPT-2 model remain frozen

Only requires learning simple visual-text alignment mapping

Model quickly converges to a reasonable solution

Stable Phase: Entering plateau around Epoch 5

Loss stabilizes around 0.3 with minimal fluctuation

Learning rate (1e-4) and AdamW optimizer appropriately set

ğŸ“Š Overall Performance Evaluation Results
Question Type	Accuracy	BLEU-1	BERTScore-F1
Closed-Ended Questions	63.03%	â€“	â€“
Open-Ended Questions	â€“	0.084	0.911
Overall	36.00%	â€“	â€“
ğŸ” Key Findings
Stable Closed Question Performance: 63% accuracy indicates reliable abnormality detection

Open Question Evaluation Divergence:

Extremely Low Surface Metrics: Exact Match 5.7%, BLEU-1 only 0.084

High Semantic Score: BERTScore-F1 reaches 0.911

Core Contradiction: Severe separation between surface form and semantic content

Page 2: Difference Analysis and Model Insights
âš™ï¸ Experimental Differences and Their Impact
1. Prefix Configuration Differences
Prefix Length: lx=10, lq/la=128 (fixed long prefixes)

Comparison: Previous work used shorter or dynamic prefixes

Impact: Long prefixes dilute visual information injection, particularly affecting open-ended answer generation

2. Language Model Limitations
Model Selection: GPT2-base (fewer parameters)

Comparison: Previous work used GPT2-XL or BioMedLM

Impact:

Limits precise word form generation ability

Insufficient lexical expression precision leads to low BLEU

Semantic content remains intact

3. Training Strategy Impact
Freezing Strategy: Language model completely frozen

Dataset Scale: Only VQA-RAD (small scale)

Optimizer: Lower learning rate, no warmup

Combined Impact: Constrains surface form accuracy while preserving semantic correctness

ğŸ’¡ Core Insights and Evaluation Reflections
ğŸ”„ Evaluation Metric Sensitivity Analysis
Metric Type	Sensitivity	Medical VQA Applicability
BLEU	Highly sensitive to word-level differences and short answer length	May underestimate model's clinical reasoning ability
BERTScore	Captures semantic similarity, considers synonyms	More comprehensively reflects clinical relevance
Accuracy	Directly effective for closed questions	Cannot assess open question quality
ğŸ“ˆ Performance Feature Summary
High BERTScore-F1: Model captures clinically relevant semantics even with different word forms

Low Surface Metrics: Caused by prefix length, model capacity, freezing strategy, dataset scale

Closed Question Robustness: Binary decision tasks not significantly affected



## æ€§èƒ½å·®è·ä¸è¡¨è¾¾è´¨é‡
ç¬¬1é¡µï¼šRQ1 - åŸºçº¿æ¨¡å‹ä¸ç”Ÿæˆå¼æ¨¡å‹çš„æ€§èƒ½å·®è·
ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ
æ¨¡å‹ç±»å‹	æµ‹è¯•å‡†ç¡®ç‡	ç›¸å¯¹æå‡	å…³é”®ç‰¹å¾
åŸºçº¿æ¨¡å‹ (ResNet-LSTM)	çº¦33%	-	æ¥è¿‘äºŒå…ƒåˆ†ç±»éšæœºçŒœæµ‹
ç”Ÿæˆå¼VLM (CLIP+GPT-2)	63.03%	+30ä¸ªç™¾åˆ†ç‚¹	è¯Šæ–­æ­£ç¡®æ€§æ˜¾è‘—æå‡
ğŸ” æ€§èƒ½å·®è·åŸå› åˆ†æ
1. è¡¨å¾å­¦ä¹ å·®å¼‚
åŸºçº¿æ¨¡å‹é—®é¢˜:

ä¸¥é‡è¿‡æ‹Ÿåˆç°è±¡

è®­ç»ƒå‡†ç¡®ç‡é«˜ä½†æµ‹è¯•æ€§èƒ½å¤§å¹…ä¸‹é™

ä»å°æ•°æ®é›†ä»å¤´è®­ç»ƒï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™

ç”Ÿæˆå¼æ¨¡å‹ä¼˜åŠ¿:

ä½¿ç”¨å†»ç»“çš„å¤§è§„æ¨¡é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ

CLIPï¼ˆè§†è§‰ç¼–ç ï¼‰+ GPT-2ï¼ˆè¯­è¨€ç”Ÿæˆï¼‰

è§†è§‰-è¯­ä¹‰å¯¹é½æä¾›å¼ºå½’çº³åç½®

2. ä½èµ„æºæ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›
æ•°æ®é›†é™åˆ¶: VQA-RADè§„æ¨¡è¾ƒå°

é¢„è®­ç»ƒä¼˜åŠ¿: å³ä½¿ä¸æ›´æ–°éª¨å¹²ç½‘ç»œå‚æ•°ï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä»å¯è¿ç§»

å¯¹é½å­¦ä¹ : æ˜ å°„ç½‘ç»œå­¦ä¹ æœ‰æ•ˆçš„è·¨æ¨¡æ€å¯¹é½ï¼Œæå‡ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°

ğŸ’¡ æ ¸å¿ƒç»“è®º
ç”Ÿæˆå¼æ–¹æ³•æ˜¾è‘—ä¼˜è¶Š: åœ¨å°é—­å¼åŒ»å­¦é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³

é¢„è®­ç»ƒæ˜¯å…³é”®: å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šä»èƒ½ä¿æŒè‰¯å¥½æ³›åŒ–

æ¶æ„è®¾è®¡å½±å“: å†»ç»“éª¨å¹²ç½‘ç»œ+è½»é‡æ˜ å°„ç½‘ç»œçš„ç»„åˆç­–ç•¥æœ‰æ•ˆ

ç¬¬2é¡µï¼šRQ2 - è¡¨è¾¾è´¨é‡ä¸å¹»è§‰é£é™©åˆ†æ
ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡çš„"æ‚–è®º"ç°è±¡
æŒ‡æ ‡è¡¨ç°å¯¹æ¯”
è¯„ä¼°ç»´åº¦	æŒ‡æ ‡è¡¨ç°	è§£é‡Š
è¯æ³•ä¸¥æ ¼æŒ‡æ ‡	è¡¨ç°å·®	ç²¾ç¡®åŒ¹é…å’ŒBLEUåˆ†æ•°æä½
è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡	è¡¨ç°ä¼˜	BERTScoreé«˜è¾¾0.911
ğŸ§  é”™è¯¯æ¡ˆä¾‹åˆ†æ
å…¸å‹å¤±è´¥æ¡ˆä¾‹ï¼ˆè¡¨10ï¼‰
çœŸå®ç­”æ¡ˆ	æ¨¡å‹é¢„æµ‹	æŒ‡æ ‡ç»“æœ	ä¸´åºŠè§£é‡Š
the brain	brain	BLEU-4=0.065	ä¸´åºŠå«ä¹‰å®Œå…¨ç›¸åŒ
right side	right	ç²¾ç¡®åŒ¹é…=0	ç¼ºå¤±ä¿®é¥°è¯­ï¼Œç©ºé—´æ¦‚å¿µæ­£ç¡®
diffusion weighted MRI	MRI	ç²¾ç¡®åŒ¹é…=0	æ­£ç¡®è¯†åˆ«æ¨¡æ€ï¼Œç¼ºå¤±å…·ä½“å­ç±»å‹
âš ï¸ å¹»è§‰é£é™©åˆ†ç±»
1. è½¯å¹»è§‰
ç‰¹å¾: æ”¹è¿°æˆ–æ›¿æ¢ä¸´åºŠç›¸å…³æœ¯è¯­

ç¤ºä¾‹: "brain" vs "the brain"

é£é™©ç­‰çº§: è¾ƒä½ï¼Œä¸å½±å“ä¸´åºŠå†³ç­–

åŸå› : è¯­è¨€æ¨¡å‹çš„æµç•…ç”Ÿæˆç‰¹æ€§

2. ç¡¬å¹»è§‰
ç‰¹å¾: ç¼–é€ ä¸å­˜åœ¨ç–¾ç—…æˆ–å‘ç°

ç¤ºä¾‹: é¢„æµ‹ä¸å­˜åœ¨ç—…å˜

é£é™©ç­‰çº§: è¾ƒé«˜ï¼Œå¯èƒ½å¯¼è‡´è¯¯è¯Š

æœ¬ç ”ç©¶è§‚å¯Ÿ: è¾ƒå°‘å‡ºç°ï¼Œå¾—ç›Šäºå†»ç»“GPT-2çš„è¿è´¯æ€§

ğŸ”¬ å®‰å…¨æ€§åˆ†æ
è¯­è¨€æ¨¡å‹å†»ç»“çš„ä¼˜åŠ¿
è¾“å‡ºè¿è´¯æ€§: å‡å°‘æ— æ„ä¹‰è¾“å‡º

æœ¯è¯­ä¸€è‡´æ€§: ä¿æŒåŒ»å­¦æœ¯è¯­å‡†ç¡®æ€§

é£é™©æ§åˆ¶: é™ä½ç¡¬å¹»è§‰å‘ç”Ÿæ¦‚ç‡

ä¸´åºŠé€‚ç”¨æ€§è¯„ä¼°
è¯­ä¹‰æ­£ç¡®æ€§ä¼˜å…ˆ: å³ä½¿è¯æ³•ä¸å®Œå…¨åŒ¹é…ï¼Œä¸´åºŠå«ä¹‰æ­£ç¡®å³å¯æ¥å—

ä¿®é¥°è¯­æ•æ„Ÿæ€§: ç¼ºå¤±éå…³é”®ä¿®é¥°è¯­å¯¹è¯Šæ–­å½±å“æœ‰é™

å­ç±»å‹ç‰¹å¼‚æ€§: è¯†åˆ«ä¸»æ¨¡æ€æ¯”ç²¾ç¡®å­ç±»å‹æ›´é‡è¦